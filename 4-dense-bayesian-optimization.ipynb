{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4-dense-bayesian-optimization.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"}},"cells":[{"metadata":{"id":"8wPjl9z7_Pp7","colab_type":"text"},"cell_type":"markdown","source":["# Optimize a dense network with Bayesian optimization\n","Authors: Thong Nguyen"]},{"metadata":{"id":"m7GNFjD4_SZV","colab_type":"code","outputId":"0d2b7975-306e-465f-e355-08b68edb341d","executionInfo":{"status":"ok","timestamp":1551935692801,"user_tz":-60,"elapsed":4340,"user":{"displayName":"Thong Nguyen","photoUrl":"https://lh5.googleusercontent.com/-WbzCE8O2FjQ/AAAAAAAAAAI/AAAAAAAAAa0/VFx_3EXybkY/s64/photo.jpg","userId":"00321500134070616579"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["!fusermount -u drive"],"execution_count":1,"outputs":[{"output_type":"stream","text":["fusermount: failed to unmount /content/drive: No such file or directory\n"],"name":"stdout"}]},{"metadata":{"id":"iAwGdx_d_XJ0","colab_type":"code","outputId":"45debfd6-738b-42bd-828e-86a5dda2fffc","executionInfo":{"status":"ok","timestamp":1551935695300,"user_tz":-60,"elapsed":6824,"user":{"displayName":"Thong Nguyen","photoUrl":"https://lh5.googleusercontent.com/-WbzCE8O2FjQ/AAAAAAAAAAI/AAAAAAAAAa0/VFx_3EXybkY/s64/photo.jpg","userId":"00321500134070616579"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","root_dir = \"/content/gdrive/My Drive/\"\n","base_dir = root_dir + 'machine-learning-vbscan/'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"uWFYDjFZ_Pp8","colab_type":"text"},"cell_type":"markdown","source":["## Usual setup: Loading `pandas` DataFrames, creating training and testing dataset\n"]},{"metadata":{"id":"-Z6hIEts_Pp-","colab_type":"code","colab":{}},"cell_type":"code","source":["import uproot\n","import numpy as np\n","import pandas as pd\n","import h5py\n","import torch\n","import torch.nn as nn\n","\n","import time\n","\n","# fix random seed for reproducibility\n","seed = 7\n","np.random.seed(seed)\n","\n","treename = 'HZZ4LeptonsAnalysisReduced'\n","filename = {}\n","upfile = {}\n","params = {}\n","df = {}\n","\n","filename['VV'] = base_dir+'data/ntuple_4mu_VV.root'\n","filename['bkg'] = base_dir+'data/ntuple_4mu_bkg.root'\n","\n","VARS = ['f_mass4l','f_massjj'] # choose which vars to use (2d)\n","\n","upfile['VV'] = uproot.open(filename['VV'])\n","params['VV'] = upfile['VV'][treename].arrays(VARS)\n","upfile['bkg'] = uproot.open(filename['bkg'])\n","params['bkg'] = upfile['bkg'][treename].arrays(VARS)\n","\n","df['VV'] = pd.DataFrame(params['VV'],columns=VARS)\n","df['bkg'] = pd.DataFrame(params['bkg'],columns=VARS)\n","\n","# cut out undefined variables VARS[0] and VARS[1] > -999\n","df['VV']= df['VV'][(df['VV'][VARS[0]] > -999) & (df['VV'][VARS[1]] > -999)]\n","df['bkg']= df['bkg'][(df['bkg'][VARS[0]] > -999) & (df['bkg'][VARS[1]] > -999)] \n","\n","# add isSignal variable\n","df['VV']['isSignal'] = np.ones(len(df['VV'])) \n","df['bkg']['isSignal'] = np.zeros(len(df['bkg'])) \n","\n","df_all = pd.concat([df['VV'],df['bkg']])\n","dataset = df_all.values\n","\n","NDIM = len(VARS)\n","\n","X = dataset[:,0:NDIM]\n","Y = dataset[:,NDIM]\n","\n","from sklearn.model_selection import train_test_split\n","X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.1, random_state=7)\n","X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.2, random_state=7)\n","\n","# preprocessing: standard scalar\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler().fit(X_train_val)\n","X_test = scaler.transform(X_test)\n","X_train = scaler.transform(X_train)\n","X_val = scaler.transform(X_val)\n","\n","# Dataset and DataLoader\n","from torch.utils.data import Dataset, DataLoader\n","\n","class HiggsDataset(Dataset):\n","  def __init__(self, xdata, label):\n","      self.xdata = torch.from_numpy(xdata).float()\n","      self.label = torch.from_numpy(label).float()\n","\n","  def __len__(self):\n","      return len(self.xdata)\n","\n","  def __getitem__(self, idx):\n","      return self.xdata[idx], self.label[idx]\n","    \n","train_data = HiggsDataset(X_train, Y_train)\n","train_loader = DataLoader(dataset=train_data, batch_size=1024, shuffle=True)\n","\n","# Get validation data ready\n","val_data = torch.from_numpy(X_val).float()\n","val_label = torch.from_numpy(Y_val).float()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m1_1ZGba_PqC","colab_type":"code","outputId":"13e20b4f-9010-440d-a24e-817264103f74","executionInfo":{"status":"ok","timestamp":1551935722566,"user_tz":-60,"elapsed":4799,"user":{"displayName":"Thong Nguyen","photoUrl":"https://lh5.googleusercontent.com/-WbzCE8O2FjQ/AAAAAAAAAAI/AAAAAAAAAa0/VFx_3EXybkY/s64/photo.jpg","userId":"00321500134070616579"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["!pip install scikit-optimize "],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting scikit-optimize\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n","\u001b[K    100% |████████████████████████████████| 81kB 2.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from scikit-optimize) (1.14.6)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python2.7/dist-packages (from scikit-optimize) (0.20.3)\n","Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python2.7/dist-packages (from scikit-optimize) (1.1.0)\n","Installing collected packages: scikit-optimize\n","Successfully installed scikit-optimize-0.5.2\n"],"name":"stdout"}]},{"metadata":{"id":"PtQYmlRd_PqB","colab_type":"text"},"cell_type":"markdown","source":["## Optimize the hyperparameters of the model\n","The hyperperparameters of the model that we weill optimize are the number of hidden layers `num_hidden`, the number of nodes in each layer `initial_node`, and the fraction of dropout `dropout`."]},{"metadata":{"id":"Ib_ax4Rg_PqL","colab_type":"text"},"cell_type":"markdown","source":["## Visualize the improvement\n","Let's see how Bayesian optimization improves the accuracy"]},{"metadata":{"scrolled":false,"id":"Mlp6vyCf_PqH","colab_type":"code","outputId":"c186db09-44b5-4d15-edfb-37dcd1f4c27d","executionInfo":{"status":"ok","timestamp":1550075040567,"user_tz":-60,"elapsed":106968,"user":{"displayName":"Thong Nguyen","photoUrl":"https://lh5.googleusercontent.com/-WbzCE8O2FjQ/AAAAAAAAAAI/AAAAAAAAAa0/VFx_3EXybkY/s64/photo.jpg","userId":"00321500134070616579"}},"colab":{"base_uri":"https://localhost:8080/","height":765}},"cell_type":"code","source":["from skopt import gp_minimize\n","from skopt.space import Real, Integer\n","from skopt.utils import use_named_args\n","\n","def build_custom_model(num_hiddens=2, initial_node=50, \n","                          dropout=0.5):\n","  \n","    modules = []\n","    modules.append(nn.Linear(NDIM, initial_node))\n","    modules.append(nn.ReLU())\n","      \n","    for i in range(num_hiddens):\n","      previous_dim = max(1,int(round(initial_node/np.power(2,i))))\n","      next_dim = max(1,int(round(initial_node/np.power(2,i+1))))\n","      modules.append(nn.Linear(previous_dim, next_dim))\n","      modules.append(nn.Dropout(p=dropout, inplace=True))\n","      modules.append(nn.ReLU())\n","      \n","    # Last layer has output size 1\n","    latest_dim = max(1,int(round(initial_node/np.power(2,num_hiddens))))\n","    modules.append(nn.Linear(latest_dim,1))\n","    modules.append(nn.Sigmoid())\n","    model = nn.Sequential(*modules)\n","    return model\n","\n","def train(model, learning_rate = 1e-3):\n","  \n","    losses, val_losses = [], []\n","    min_loss, stale_epochs = 100., 0\n","    # Use Binary Cross Entropy as our loss function.\n","    loss_fn = torch.nn.BCELoss()\n","    \n","    # Optimize the model parameters using the Adam optimizer.\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    for t in xrange(500):\n","      batch_loss = []\n","\n","      for x_train, y_train in train_loader:\n","\n","          # Forward pass: make a prediction for each x event in batch b.\n","          y_pred = model(x_train)\n","\n","          # Get the labels.\n","          label = y_train\n","          y = label.view_as(y_pred)  # reshape label data to the shape of y_pred\n","\n","          # Compute and print loss.\n","          loss = loss_fn(y_pred, y)\n","          batch_loss.append(loss.item())\n","\n","          # Before the backward pass, use the optimizer object to zero all of the\n","          # gradients for the variables it will update (which are the learnable\n","          # weights of the model). This is because by default, gradients are\n","          # accumulated in buffers( i.e, not overwritten) whenever .backward()\n","          # is called. Checkout docs of torch.autograd.backward for more details.\n","          optimizer.zero_grad()\n","\n","          # Backward pass: compute gradient of the loss with respect to model\n","          # parameters\n","          loss.backward()\n","\n","          # Calling the step function on an Optimizer makes an update to its\n","          # parameters\n","          optimizer.step()\n","\n","      # Let's look at the validation set.\n","\n","      # Torch keeps track of each operation performed on a Tensor, so that it can take the gradient later.\n","      # We don't need to store this information when looking at validation data, so turn it off with\n","      # torch.no_grad().\n","      with torch.no_grad():\n","\n","          # Forward pass on validation set.\n","          output = model(val_data)\n","\n","          # Get labels and compute loss again\n","          val_y = val_label.view_as(output)\n","          val_loss = loss_fn(output, val_y)\n","          \n","      this_loss = np.mean(batch_loss)\n","      losses.append(this_loss)\n","      this_val_loss = val_loss.item()\n","      val_losses.append(this_val_loss)\n","\n","      # Monitor the loss function to prevent overtraining.\n","      if stale_epochs > 20:\n","          break\n","\n","      if this_val_loss < min_loss:\n","          min_loss = this_val_loss\n","          stale_epochs = 0\n","      else:\n","          stale_epochs += 1\n","\n","      # Return the best validation loss as the figure of merit\n","    return min(val_losses)\n","\n","space  = [Integer(1, 3, name='hidden_layers'),\n","          Integer(5, 100, name='initial_nodes'),\n","          Real(0.0,0.9,name='dropout'),\n","          Real(10**-5, 10**-1, \"log-uniform\", name='learning_rate'),\n","          ]\n","\n","@use_named_args(space)\n","def objective(**X):\n","    print(\"New configuration: {}\".format(X))\n","\n","    model = build_custom_model(num_hiddens=X['hidden_layers'], initial_node=X['initial_nodes'], \n","                      dropout=X['dropout'])\n","\n","    fom = train(model, learning_rate=X['learning_rate'])\n","    \n","    return fom\n","\n","begt = time.time()\n","res_gp = gp_minimize(objective, space, n_calls=10, n_random_starts=5, random_state=3, verbose=True)\n","print(\"Finish optimization in {}s\".format(time.time()-begt))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Iteration No: 1 started. Evaluating function at random point.\n","New configuration: {'dropout': 0.10919572308469448, 'hidden_layers': 1, 'learning_rate': 0.001893412804339247, 'initial_nodes': 85}\n","Iteration No: 1 ended. Evaluation done at random point.\n","Time taken: 10.6963\n","Function value obtained: 0.0485\n","Current minimum: 0.0485\n","Iteration No: 2 started. Evaluating function at random point.\n","New configuration: {'dropout': 0.03656766330092095, 'hidden_layers': 2, 'learning_rate': 9.807384199410213e-05, 'initial_nodes': 7}\n","Iteration No: 2 ended. Evaluation done at random point.\n","Time taken: 63.7356\n","Function value obtained: 0.6725\n","Current minimum: 0.0485\n","Iteration No: 3 started. Evaluating function at random point.\n","New configuration: {'dropout': 0.13083312966786248, 'hidden_layers': 1, 'learning_rate': 0.0006496646024457744, 'initial_nodes': 71}\n","Iteration No: 3 ended. Evaluation done at random point.\n","Time taken: 18.3546\n","Function value obtained: 0.0487\n","Current minimum: 0.0485\n","Iteration No: 4 started. Evaluating function at random point.\n","New configuration: {'dropout': 0.4433723325414698, 'hidden_layers': 1, 'learning_rate': 0.04499930257575027, 'initial_nodes': 39}\n","Iteration No: 4 ended. Evaluation done at random point.\n","Time taken: 12.3581\n","Function value obtained: 0.0638\n","Current minimum: 0.0485\n","Iteration No: 5 started. Evaluating function at random point.\n","New configuration: {'dropout': 0.36212457920306235, 'hidden_layers': 3, 'learning_rate': 0.001625877427086297, 'initial_nodes': 97}\n","Iteration No: 5 ended. Evaluation done at random point.\n","Time taken: 11.0625\n","Function value obtained: 0.0574\n","Current minimum: 0.0485\n","Iteration No: 6 started. Searching for the next optimal point.\n","New configuration: {'dropout': 0.9, 'hidden_layers': 3, 'learning_rate': 1e-05, 'initial_nodes': 5}\n","Iteration No: 6 ended. Search finished for the next optimal point.\n","Time taken: 72.2667\n","Function value obtained: 0.8662\n","Current minimum: 0.0485\n","Iteration No: 7 started. Searching for the next optimal point.\n","New configuration: {'dropout': 0.03432388783872939, 'hidden_layers': 3, 'learning_rate': 0.03616781532572513, 'initial_nodes': 57}\n","Iteration No: 7 ended. Search finished for the next optimal point.\n","Time taken: 7.9024\n","Function value obtained: 0.0430\n","Current minimum: 0.0430\n","Iteration No: 8 started. Searching for the next optimal point.\n","New configuration: {'dropout': 0.0, 'hidden_layers': 1, 'learning_rate': 1e-05, 'initial_nodes': 100}\n"],"name":"stdout"}]},{"metadata":{"id":"m2YT2JCY_PqM","colab_type":"code","outputId":"7032aeed-6ffb-43fe-ed86-852c4aeb02a0","executionInfo":{"status":"error","timestamp":1550074924714,"user_tz":-60,"elapsed":1100,"user":{"displayName":"Thong Nguyen","photoUrl":"https://lh5.googleusercontent.com/-WbzCE8O2FjQ/AAAAAAAAAAI/AAAAAAAAAa0/VFx_3EXybkY/s64/photo.jpg","userId":"00321500134070616579"}},"colab":{"base_uri":"https://localhost:8080/","height":181}},"cell_type":"code","source":["from skopt.plots import plot_convergence\n","plot_convergence(res_gp)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-6-69919e5573be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_convergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_gp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'res_gp' is not defined"]}]},{"metadata":{"id":"IWSVtgzT_PqP","colab_type":"code","outputId":"fceeb600-4b42-46e7-ac54-4f4441953fc1","executionInfo":{"status":"ok","timestamp":1550030512157,"user_tz":-60,"elapsed":554,"user":{"displayName":"CMS DAS","photoUrl":"","userId":"06521840035652546617"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["print(\"Best parameters: \\\n","\\nbest_hidden_layers = {} \\\n","\\nbest_initial_nodes = {} \\\n","\\nbest_dropout = {} \\\n","\\nbest_batch_size = {} \\\n","\\nbest_learning_rate = {}\".format(res_gp.x[0],\n","                                 res_gp.x[1],\n","                                 res_gp.x[2],\n","                                 res_gp.x[3],\n","                                 res_gp.x[4]))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Best parameters: \n","best_hidden_layers = 2 \n","best_initial_nodes = 100 \n","best_dropout = 0.0 \n","best_batch_size = 500 \n","best_learning_rate = 0.1\n"],"name":"stdout"}]},{"metadata":{"id":"6f1asrGm_PqZ","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}